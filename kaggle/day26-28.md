### ensembling - bagging / 集成機器學習方法

- Exmined ensem methods
    - Averaging(or blending)
        - 兩個模型，各自有一半的模型效果，看 score 比之前各自單獨跑的效果好(模型1 + 模型2) / 2
    - Weighted averaging
        - 可以改成加權，看score (模型1 * 0.7 + 模型2 * 0.3)
    - Conditional averaging
        - 截長補短，只取個模型好的部分
    - Bagging裝袋法
        - 同一模型的稍有不同版本進行平均，以提高與測能力
        - Errors due to Bias. 偏差引起的誤差, 導致 underfitting
        - Errors due Variance. 方差引起的誤差, 導致 overfitting
        - [model_complexity](model_complexity.png)
    - Boosting提升法
        - 提升法是把之前的預測透過權重或殘差整合成一個綜合模型的方法
        - 重點在極小化error 的兩種作法
            1. 錯誤率的弱分類器放⼤，是一種自適應提升
            2. 而殘差調整成新目標值, 則是梯度提升機
        - Weight based boosting
            - 權重以實際與預測之絕對差加上1
        - Residual based boosting
            - 以殘差取代變成新的實際值
    - Stacking堆疊
        - 以先前模型的預測結果來影響新模型
        - 步驟:
            - Spliting : 分成 training 跟 validation
            - Train : 舉例分別 fit 以 RandomForestregressor 跟 LeanearRegression 來產生 base model.
            - Make Prediction : 將預測結果當作是下一階段的 meta learner的 meta model.
            - train a higher level learner : fit 新的 meta model.
        [stack](stack.png)
    - Stacknet
        -  一般來說訓練是在訓練參數跟特徵(也就是學生)，stacknet的概念是訓練出好老師
        - 可伸缩很靈活，它可以將學習器一層一層地堆砌起來，同一個level內可並行好幾種模型，形成一個網狀的結構
        - 步驟說明
            1. 數據分成training data跟valid data
            2. valid data再分成minin train跟mini volid
            3. 進行 K-fold(可參考 day13)，這邊折4折，所以K=4
            4. 最後將所有data(反灰)一次餵進model跑test data.所以有5個預測model，這時可以用本次結果堆疊，再來一次K-fold.
- 集成類別
    - 資料集成
        - Bagging/Boosting : 使用不同訓練資料 + 同一種模型，多次估計的結果和成最終預測
    - 模型與特徵集成
        - Voting/Blending/Stacking : 使用同一資料 + 不同模型，合出不同預測結果
- 小叮嚀
    - Go out there, apply what you've learned. Choose a competition. 快去參加個競賽
    - Don't be demoralized if you see there's still a gap with the top people, because it does take some time to adjust.You need to learn the dynamics. 別因為跟高手差很多就洩氣, 有時就需要多點時間
    - Something that has always helped me is to save my code, and try and improve it. 把寫過的 code 存起來, 想辦法改善, 然後下次競賽就會讓自己表現更好, 更強
    - seek collaborations. 參與協作, 每個人觀點不同, 可以彼此補強
    - need to be connected with forums, and codes, and kernels.. 不能不閱讀就進步, 要持續學習, 閱讀
- Base line
- State of the art
